{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31892,"status":"ok","timestamp":1743672704718,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"EqkR1AKn4SUC","outputId":"af2643f8-c796-4d22-cbc0-b2847c0e1ae7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","\u001b[33mWARNING: Skipping kerastuner as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting keras-tuner\n","  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n","Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (2.0.2)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import tensorflow as tf\n","\n","!pip uninstall -y kerastuner\n","!pip install keras-tuner\n","import numpy as np\n","import keras_tuner as kt\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","!pip install numba\n","from numba import cuda\n","import time\n","\n","import os\n","import random\n","# Set random seeds for reproducibility\n","def set_random_seeds(seed=42):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    tf.random.set_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","set_random_seeds()\n","# Ensure deterministic operations\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n","os.environ['OMP_NUM_THREADS'] = '1'\n","os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n","os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n","os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n","\n","\n","tf.keras.utils.set_random_seed(42)\n","tf.config.experimental.enable_op_determinism()\n","# Though with all this step, tere are still controlled noise which actually can help with robustness of the model.\n","# https://github.com/NVIDIA/framework-reproducibility/blob/master/doc/d9m/README.md\n","# https://github.com/NVIDIA/framework-reproducibility/blob/master/doc/d9m/tensorflow.md"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32639,"status":"ok","timestamp":1743672737360,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"9w83ik8339FW","outputId":"61ae4903-1458-434f-896f-74f44c1ac15c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data has been loaded from /content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_train_complete.pkl\n","Data has been loaded from /content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_val_complete.pkl\n","Data has been loaded from /content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_test_complete.pkl\n"]}],"source":["file_path_train = f'/content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_train_complete.pkl'\n","with open(file_path_train, 'rb') as file:\n","    train_data = pickle.load(file)\n","print(f\"Data has been loaded from {file_path_train}\")\n","\n","file_path_val = f'/content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_val_complete.pkl'\n","with open(file_path_val, 'rb') as file:\n","    val_data = pickle.load(file)\n","print(f\"Data has been loaded from {file_path_val}\")\n","\n","file_path_test = f'/content/drive/MyDrive/Colab Notebooks/GNN/Input_data/final_preprocessed_test_complete.pkl'\n","with open(file_path_test, 'rb') as file:\n","    test_data = pickle.load(file)\n","print(f\"Data has been loaded from {file_path_test}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1209,"status":"ok","timestamp":1743672738574,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"NRvuqBQ9NM47","outputId":"8540a743-e2cb-4492-d0e5-b85b0f97a969"},"outputs":[{"name":"stdout","output_type":"stream","text":["(74, 551, 20, 2) (74, 551)\n","(24, 551, 20, 2) (24, 551)\n","(24, 551, 20, 2) (24, 551)\n"]}],"source":["x_train = train_data[0][:, :, :, :2]\n","y_train = train_data[1][:]\n","print(x_train.shape, y_train.shape)\n","\n","x_val = val_data[0][:, :, :, :2]\n","y_val = val_data[1][:]\n","print(x_val.shape, y_val.shape)\n","\n","x_test = test_data[0][:, :, :, :2]\n","y_test = test_data[1][:]\n","print(x_test.shape, y_test.shape)\n","\n","# Ensure data types are compatible with TensorFlow\n","x_train = tf.cast(x_train, dtype=tf.float32)\n","y_train = tf.cast(y_train, dtype=tf.float32)\n","x_val = tf.cast(x_val, dtype=tf.float32)\n","y_val = tf.cast(y_val, dtype=tf.float32)\n","x_test = tf.cast(x_test, dtype=tf.float32)\n","y_test = tf.cast(y_test, dtype=tf.float32)"]},{"cell_type":"markdown","metadata":{"id":"tl-f1F9y8E1a"},"source":["Loading Fixed Adj Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1098,"status":"ok","timestamp":1743672739680,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"ri4xbc7HFj8S","outputId":"e5ad4f78-be7c-48db-d7b8-d0038ad2730c"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['lookup_dict', 'total_list_of_property', 'adj_matrices'])\n","['followsTicker', 'subsidiaryTicker', 'parentTicker', 'ownedByTicker', 'ownerOfTicker', 'industryLabel', 'legalformLabel', 'founded_byLabel', 'prod_mat_producedLabel', 'complies_withLabel', 'GISC_industry']\n","The relation is followsTicker in np array form\n","The relation is subsidiaryTicker in np array form\n","The relation is parentTicker in np array form\n","The relation is ownedByTicker in np array form\n","The relation is ownerOfTicker in np array form\n","The relation is industryLabel in np array form\n","The relation is legalformLabel in np array form\n","The relation is founded_byLabel in np array form\n","The relation is prod_mat_producedLabel in np array form\n","The relation is complies_withLabel in np array form\n","The relation is GISC_industry\n"]}],"source":["file_path_f_adj_matrix = f'/content/drive/MyDrive/Colab Notebooks/GNN/combined_adj_matrix.pkl'\n","with open(file_path_f_adj_matrix, 'rb') as file:\n","    f_adj_matrix = pickle.load(file)\n","\n","print(f_adj_matrix.keys())\n","print(f_adj_matrix['total_list_of_property'])\n","relation_list = f_adj_matrix['total_list_of_property']\n","\n","relation_list_w = relation_list[0:-1]\n","adj_matrices_wiki = []\n","for i in relation_list[0:-1]:\n","  print(\"The relation is\", i, \"in np array form\")\n","  # print(f_adj_matrix['adj_matrices'][i])\n","  adj_matrices_wiki.append(f_adj_matrix['adj_matrices'][i])\n","\n","\n","print(\"The relation is\", relation_list[-1])\n","relation_list_i = [relation_list[-1]]\n","adj_matrices_industry = [f_adj_matrix['adj_matrices'][relation_list[-1]]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njFcO7C9jKQU"},"outputs":[],"source":["def compute_irr_top1(predictions, ground_truth, sample_weight=None):\n","    top1_indices = tf.argmax(predictions, axis=-1)\n","    top1_returns = tf.gather(ground_truth, top1_indices, batch_dims=1)\n","    irr = tf.reduce_mean(top1_returns)\n","    return irr\n","\n","def pointwise_regression_loss(predictions, targets, sample_weight=None):\n","    point_wise_loss = tf.reduce_mean(tf.square(predictions - targets), axis=1)\n","    total_loss = tf.reduce_sum(point_wise_loss)\n","    return total_loss\n","\n","def compute_mrr_top1(predictions, ground_truth, sample_weight=None):\n","    top1_indices = tf.argmax(predictions, axis=-1)\n","    top1_ground_truth = tf.gather(ground_truth, top1_indices, batch_dims=1)\n","    sorted_ground_truth_indices = tf.argsort(ground_truth, axis=-1, direction='DESCENDING')\n","    ranks = tf.argsort(sorted_ground_truth_indices, axis=-1) + 1\n","    top1_ranks = tf.gather(ranks, top1_indices, batch_dims=1)\n","    reciprocal_ranks = tf.where(top1_ground_truth > 0, 1.0 / tf.cast(top1_ranks, tf.float32), 0.0)\n","    overall_mrr = tf.reduce_mean(reciprocal_ranks)\n","    return overall_mrr"]},{"cell_type":"markdown","metadata":{"id":"XR4-PK9R354v"},"source":["Using deteministic"]},{"cell_type":"markdown","metadata":{"id":"2IPV3JB9m4QG"},"source":["Initial Using Fully Dense Matrix for Attention -Working FIne but the tuner and rerun result are not consistent."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47004,"status":"ok","timestamp":1743672786719,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"o8uV2oJ6kxEs","outputId":"42c0e245-f6f7-420a-e187-eda208457d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Physical devices cannot be modified after being initialized\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-6-b830354b893a>:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  import kerastuner as kt\n"]},{"name":"stdout","output_type":"stream","text":["Reloading Tuner from /content/drive/MyDrive/Colab Notebooks/GNN/Baseline 3: GAT with fixed graph and GRU/BSM3_HP_(Wiki n Industry)_GPU_Mem_lim_w_growth_1/tuner0.json\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import LeakyReLU, Dense\n","from tensorflow.keras.regularizers import l2\n","import kerastuner as kt\n","import gc\n","import random\n","import os\n","\n","tf.config.run_functions_eagerly(False)\n","\n","\n","# Set GPU memory limit and enable memory growth\n","def set_gpu_memory_limit(memory_limit):\n","    gpus = tf.config.list_physical_devices('GPU')\n","    if gpus:\n","        try:\n","            # Enable memory growth\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","\n","            # Set memory limit\n","            tf.config.set_logical_device_configuration(\n","                gpus[0],\n","                [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n","            )\n","            print(f\"Set GPU memory limit to {memory_limit} MB\")\n","        except RuntimeError as e:\n","            print(e)\n","\n","# Define the GAT model and other necessary classes/functions\n","class MultiHeadGATLayer(tf.keras.layers.Layer):\n","    def __init__(self, gru_units, num_heads, head_dim, adj_matrices):\n","        super(MultiHeadGATLayer, self).__init__()\n","        self.gru_units = gru_units\n","        self.num_heads = num_heads\n","        self.head_dim = head_dim\n","        self.num_relations = len(adj_matrices)\n","        self.adj_matrices = [tf.cast(adj, dtype=tf.float32) for adj in adj_matrices]\n","        self.gat_output_shape = None\n","\n","        self.W1 = [\n","            [self.add_weight(\n","                shape=(gru_units, self.head_dim),\n","                initializer=tf.keras.initializers.RandomNormal(seed=42),  # Seeded initializer\n","                trainable=True,\n","                name=f\"W1_rel{r}_head{h}\"\n","            ) for h in range(num_heads)]\n","            for r in range(self.num_relations)\n","        ]\n","\n","        self.r = [\n","            [self.add_weight(\n","                shape=(2 * self.head_dim, 1),\n","                initializer=tf.keras.initializers.RandomNormal(seed=42),  # Seeded initializer\n","                trainable=True,\n","                name=f\"r_rel{r}_head{h}\"\n","            ) for h in range(self.num_heads)]\n","            for r in range(self.num_relations)\n","        ]\n","\n","        self.leaky_relu = LeakyReLU(negative_slope=0.2)\n","\n","    def build(self, input_shape):\n","        self.num_stocks = input_shape[1]\n","        super(MultiHeadGATLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        head_outputs = []\n","        batch_size = tf.shape(inputs)[0]\n","\n","        for r in range(self.num_relations):\n","            relation_head_outputs = []\n","            for h in range(self.num_heads):\n","                W1_rh = self.W1[r][h]\n","                r_rh = self.r[r][h]\n","\n","                h_i = tf.matmul(inputs, W1_rh)\n","                h_j = tf.expand_dims(h_i, axis=2)\n","                h_j = tf.tile(h_j, [1, 1, self.num_stocks, 1])\n","                h_i_expanded = tf.expand_dims(h_i, axis=1)\n","                h_i_expanded = tf.tile(h_i_expanded, [1, self.num_stocks, 1, 1])\n","                concat_features = tf.concat([h_i_expanded, h_j], axis=-1)\n","                concat_features_reshaped = tf.reshape(concat_features, [-1, 2 * self.head_dim])\n","\n","                scores = self.leaky_relu(tf.matmul(concat_features_reshaped, r_rh))\n","                scores = tf.reshape(scores, [batch_size, self.num_stocks, self.num_stocks])\n","\n","                adj_matrix = tf.expand_dims(self.adj_matrices[r], axis=0)\n","                adj_matrix = tf.tile(adj_matrix, [batch_size, 1, 1])\n","                masked_scores = tf.where(adj_matrix > 0, scores, -1e9 * tf.ones_like(scores))\n","                attention_weights = tf.nn.softmax(masked_scores, axis=-1)\n","                output = tf.matmul(attention_weights, h_i)\n","                relation_head_outputs.append(output)\n","\n","            relation_output = tf.concat(relation_head_outputs, axis=-1)\n","            head_outputs.append(relation_output)\n","\n","        final_output = tf.reduce_mean(tf.stack(head_outputs, axis=-1), axis=-1)\n","        self.gat_output_shape = final_output.shape\n","        return tf.nn.relu(final_output)\n","\n","    def compute_output_shape(self, input_shape):\n","        return self.gat_output_shape\n","\n","class MultiStockGRUModel(tf.keras.Model):\n","    def __init__(self, gru_units, dense_units, dense_units2, num_heads, head_dim,\n","                 adj_matrices_w, adj_matrices_i, number_of_layers, dense_initializer, dense_activation, l2_lambda=0.005):\n","        super(MultiStockGRUModel, self).__init__()\n","        self.gru_units = gru_units\n","        self.num_dense_layers = number_of_layers\n","        self.l2_lambda = l2_lambda\n","        self.gru_layers = []\n","        self.dense_units = dense_units\n","        self.dense_units2 = dense_units2\n","\n","        # Build the dense layers stack with kernel_regularizer added\n","        self.dense_layers = []\n","\n","        for _ in range(number_of_layers - 1):\n","            if dense_activation == 'leaky_relu':\n","                self.dense_layers.append(\n","                    tf.keras.Sequential([\n","                        Dense(dense_units, activation=None, kernel_initializer=dense_initializer,\n","                              kernel_regularizer=l2(l2_lambda)),\n","                        LeakyReLU(negative_slope =0.2)\n","                    ])\n","                )\n","            else:\n","                self.dense_layers.append(\n","                    Dense(dense_units, activation=dense_activation, kernel_initializer=dense_initializer,\n","                          kernel_regularizer=l2(l2_lambda))\n","                )\n","        # Final dense layer (unchanged, but you can also add regularization if desired)\n","        self.dense_layers.append(\n","            Dense(dense_units2, activation=None, kernel_regularizer=l2(l2_lambda))\n","        )\n","\n","        # Initialize your custom GAT layers as before\n","        self.gat_layer_w = MultiHeadGATLayer(gru_units, num_heads, head_dim, adj_matrices_w)\n","        self.gat_layer_i = MultiHeadGATLayer(gru_units, num_heads, head_dim, adj_matrices_i)\n","\n","    def build(self, input_shape):\n","        self.num_stocks = input_shape[1]\n","        self.gru_layers = [\n","            tf.keras.layers.GRU(self.gru_units, return_sequences=False, name=f\"GRU_Stock_{i}\")\n","            for i in range(self.num_stocks)\n","        ]\n","\n","    def call(self, inputs, adj_matrices=None):\n","        batch_size = tf.shape(inputs)[0]\n","\n","        stock_outputs = []\n","        for i in range(self.num_stocks):\n","            stock_input = inputs[:, i, :, :]\n","            stock_output = self.gru_layers[i](stock_input)\n","            stock_outputs.append(stock_output)\n","\n","        fused_output = tf.stack(stock_outputs, axis=1)\n","\n","        gat_output_w = self.gat_layer_w(fused_output)\n","\n","        gat_output_i = self.gat_layer_i(fused_output)\n","\n","        gat_combined = tf.concat([gat_output_w, gat_output_i], axis=-1)\n","\n","        x = gat_combined\n","        for i, layer in enumerate(self.dense_layers):\n","            x = layer(x)\n","\n","        final_output = tf.squeeze(x, axis=-1)\n","\n","        return final_output\n","\n","\n","\n","class CustomMetric(tf.keras.metrics.Metric):\n","    def __init__(self, metric_fn, name, **kwargs):\n","        super(CustomMetric, self).__init__(name=name, **kwargs)\n","        self.metric_fn = metric_fn\n","        self.result_value = self.add_weight(name=f\"{name}_value\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=f\"{name}_count\", initializer=\"zeros\")\n","\n","    @tf.function\n","    def update_state(self, y_true, y_pred, **kwargs):\n","        value = self.metric_fn(y_pred, y_true, **kwargs)\n","        self.result_value.assign_add(value)\n","        self.count.assign_add(1.0)\n","\n","    @tf.function\n","    def result(self):\n","        return self.result_value / tf.maximum(self.count, 1.0)\n","\n","    @tf.function\n","    def reset_states(self):\n","        self.result_value.assign(0.0)\n","        self.count.assign(0.0)\n","\n","metrics = [\n","    CustomMetric(pointwise_regression_loss, name=\"pointwise_regression_loss\"),\n","    CustomMetric(compute_irr_top1, name=\"compute_irr_top1\"),\n","    CustomMetric(compute_mrr_top1, name=\"compute_mrr_top1\")\n","]\n","\n","@tf.function\n","def optimized_custom_loss_function(y_true, y_pred, model=None, alpha=1.0, lambda_=0.01):\n","    pointwise_loss = tf.reduce_mean(tf.square(y_pred - y_true))\n","    batch_size = tf.shape(y_true)[0]\n","    y_pred_diff = tf.expand_dims(y_pred, 2) - tf.expand_dims(y_pred, 1)\n","    y_true_diff = tf.expand_dims(y_true, 2) - tf.expand_dims(y_true, 1)\n","    pairwise_loss = tf.reduce_sum(tf.maximum(0.0, -y_pred_diff * y_true_diff))\n","    pairwise_loss = alpha * pairwise_loss / tf.cast(batch_size, tf.float32)\n","    l2_loss = 0.0\n","    if model is not None:\n","        l2_loss = lambda_ * tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n","    loss = pointwise_loss + pairwise_loss + l2_loss\n","    return loss\n","\n","\n","\n","class CustomHyperModel(kt.HyperModel):\n","    def build(self, hp):\n","        gru_units = hp.Choice('gru_units', [24, 36, 48])\n","        dense_units = hp.Choice('dense_units', [24, 36, 48])\n","        head_dim = hp.Choice('head_dim', [7, 10])\n","        num_heads = hp.Choice('num_heads', [8, 10])\n","        dense_units2 = 1\n","        number_of_layers = hp.Choice('number_of_layers', [2, 3, 4])\n","        dense_initializer = hp.Choice(\"dense_initializer\", ['glorot_uniform', 'he_normal', 'lecun_normal', 'random_normal'])\n","        dense_activation = hp.Choice(\"dense_activation\", ['linear', 'relu', 'leaky_relu', 'softmax'])\n","        alpha = hp.Choice('alpha', [2.0, 1.5, 1.0])\n","        lambda_ = hp.Choice('lambda', [0.015, 0.005])\n","\n","        # Initialize model with adjacency matrices (assumed to be defined)\n","        model = MultiStockGRUModel(\n","            gru_units, dense_units, dense_units2, num_heads, head_dim,\n","            adj_matrices_wiki, adj_matrices_industry,\n","            number_of_layers, dense_initializer, dense_activation, l2_lambda=0.005\n","        )\n","        model.build((None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n","\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","        model.compile(optimizer=optimizer,\n","                      loss=lambda y_true, y_pred: optimized_custom_loss_function(y_true, y_pred, model, alpha=alpha, lambda_=lambda_),\n","                      metrics=metrics)\n","        return model\n","\n","    @tf.function\n","    def run_trial(self, trial, x_train, y_train, x_val, y_val, epochs, batch_size, **kwargs):\n","        try:\n","            # Build and compile the model\n","            model = self.build(trial.hyperparameters)\n","            # Define early stopping callback\n","            early_stopping = tf.keras.callbacks.EarlyStopping(\n","                monitor='val_loss',  # Metric to monitor\n","                patience=1,          # Number of epochs with no improvement after which training will be stopped\n","                restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n","            )\n","            # Fit the model with early stopping\n","            history = model.fit(\n","                x_train, y_train,\n","                validation_data=(x_val, y_val),\n","                epochs=epochs,\n","                batch_size=batch_size,\n","                callbacks=[early_stopping],  # Add early stopping callback\n","                **kwargs\n","            )\n","        finally:\n","            # Ensure CUDA device is reset after each trial\n","            tf.keras.backend.clear_session()\n","            gc.collect()\n","        return history\n","\n","# Set GPU memory limit to 14.5 GB (14500 MB) and enable memory growth\n","set_gpu_memory_limit(14800)\n","\n","tuner = kt.RandomSearch(\n","    CustomHyperModel(),\n","    objective=kt.Objective('compute_irr_top1', direction='max'),\n","    max_trials=70,\n","    executions_per_trial=3,# Run 3 trials to leverage the non-deterministic nature of validation dataset as a proxy for a better result for test dataset.\n","    directory='/content/drive/MyDrive/Colab Notebooks/GNN/Baseline 3: GAT with fixed graph and GRU/',\n","    project_name=f'BSM3_HP_(Wiki n Industry)_GPU_Mem_lim_w_growth_1')\n","\n","# Start hyperparameter search (adjacency matrices are passed during model building)\n","tuner.search(\n","    x_train,\n","    y_train,\n","    epochs=6,\n","    validation_data=(x_val, y_val),\n","    verbose=1,  # Ensure verbose is set to 1 to print epoch status\n","    batch_size=2)\n","\n","\n","class CustomModel(tf.keras.Model):\n","    def __init__(self, loaded_model):\n","        super(CustomModel, self).__init__()\n","        self.loaded_model = loaded_model\n","\n","    def call(self, inputs):\n","        return self.loaded_model.signatures['serving_default'](inputs)['output_0']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1019099,"status":"ok","timestamp":1743665800646,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"93AzO5b5Lj6A","outputId":"9aab9fd4-a972-4e45-e8ef-b3fc8031dc85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 3s/step - compute_irr_top1: 5.5543e-04 - compute_mrr_top1: 0.0040 - loss: 352.1924 - pointwise_regression_loss: 0.0058 - val_compute_irr_top1: 0.0047 - val_compute_mrr_top1: 0.0143 - val_loss: 141.2309 - val_pointwise_regression_loss: 0.0017\n","Epoch 2/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - compute_irr_top1: 0.0148 - compute_mrr_top1: 0.0162 - loss: 123.2733 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: -0.0018 - val_compute_mrr_top1: 0.0082 - val_loss: 86.8406 - val_pointwise_regression_loss: 0.0017\n","Epoch 3/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - compute_irr_top1: 0.0185 - compute_mrr_top1: 0.0270 - loss: 79.7178 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 0.0041 - val_compute_mrr_top1: 0.0122 - val_loss: 63.1171 - val_pointwise_regression_loss: 0.0017\n","Epoch 4/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - compute_irr_top1: 0.0161 - compute_mrr_top1: 0.0309 - loss: 57.9348 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 2.7199e-04 - val_compute_mrr_top1: 0.0083 - val_loss: 48.8463 - val_pointwise_regression_loss: 0.0017\n","Epoch 5/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - compute_irr_top1: 0.0169 - compute_mrr_top1: 0.0671 - loss: 45.2726 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 0.0022 - val_compute_mrr_top1: 0.0131 - val_loss: 40.6652 - val_pointwise_regression_loss: 0.0017\n","Epoch 6/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - compute_irr_top1: 0.0144 - compute_mrr_top1: 0.0295 - loss: 37.3631 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 0.0019 - val_compute_mrr_top1: 0.0090 - val_loss: 35.5095 - val_pointwise_regression_loss: 0.0017\n","Epoch 7/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - compute_irr_top1: 0.0086 - compute_mrr_top1: 0.0224 - loss: 32.3886 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 0.0018 - val_compute_mrr_top1: 0.0109 - val_loss: 30.7575 - val_pointwise_regression_loss: 0.0017\n","Epoch 8/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - compute_irr_top1: 0.0069 - compute_mrr_top1: 0.0186 - loss: 28.9229 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: -0.0016 - val_compute_mrr_top1: 0.0083 - val_loss: 27.6371 - val_pointwise_regression_loss: 0.0017\n","Epoch 9/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - compute_irr_top1: 0.0098 - compute_mrr_top1: 0.0252 - loss: 25.7049 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 6.5109e-04 - val_compute_mrr_top1: 0.0092 - val_loss: 25.3052 - val_pointwise_regression_loss: 0.0017\n","Epoch 10/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - compute_irr_top1: 0.0108 - compute_mrr_top1: 0.0112 - loss: 23.5766 - pointwise_regression_loss: 0.0022 - val_compute_irr_top1: 0.0062 - val_compute_mrr_top1: 0.0181 - val_loss: 23.4212 - val_pointwise_regression_loss: 0.0017\n","Training Time: 1015.28 seconds\n"]}],"source":["# Extract best hyperparameters from the tuner\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Retrain the model with the best hyperparameters\n","set_random_seeds()  # Ensure reproducibility\n","\n","\n","model = CustomHyperModel().build(best_hps)\n","\n","# Start counting time\n","start_time = time.time()\n","\n","history = model.fit(\n","    x_train, y_train,\n","    validation_data=(x_val, y_val),\n","    epochs=10,\n","    batch_size=2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)]\n",")\n","\n","# Stop counting time\n","end_time = time.time()\n","\n","# Calculate elapsed time\n","elapsed_time = end_time - start_time\n","print(f\"Training Time: {elapsed_time:.2f} seconds\")\n","\n","save_path = '/content/drive/MyDrive/Colab Notebooks/GNN/Baseline 3: GAT with fixed graph and GRU/saved_model1'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153976,"status":"ok","timestamp":1743665954631,"user":{"displayName":"William William","userId":"14956234435432560456"},"user_tz":-480},"id":"Xi6HkWLcSUmR","outputId":"f2a2bba7-c610-4859-aef9-34e58f8f5ed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 417ms/step - compute_irr_top1: 0.0035 - compute_mrr_top1: 0.0163 - loss: 5.2488 - pointwise_regression_loss: 0.0017\n","Test Results: [5.663527011871338, 0.0018109874799847603, 0.003927816636860371, 0.017325112596154213]\n"]}],"source":["# Save the model in TensorFlow SavedModel format\n","tf.saved_model.save(model, save_path)\n","\n","tf.keras.backend.clear_session()\n","gc.collect()\n","\n","# Load the model with custom objects\n","loaded_model = tf.saved_model.load(save_path)\n","\n","custom_model = CustomModel(loaded_model)\n","custom_model.compile(optimizer='adam', loss=optimized_custom_loss_function, metrics=metrics)\n","\n","# Assuming x_test and y_test are defined\n","test_results = custom_model.evaluate(x_test, y_test, batch_size=2)\n","print(f\"Test Results: {test_results}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"mount_file_id":"1z5lfyHLDgPmt_fTXpB2VeFWFQg4bpaLJ","authorship_tag":"ABX9TyMRGVJ3howp278txLxWeEXW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}